
[Installation on windows:
  1. https://developer.hashicorp.com/terraform/downloads
  2. Use GIT Bash
  3. echo $PATH
  4. Add extracted folder of terraform to any of the above path 
      ex: /c/Users/anuka/bin
  
terraform -version
Terraform is an open-source infrastructure as code software tool that enables users to provision and manage infrastructure resources through declarative configuration files. It supports multiple cloud providers, including AWS, Azure, and Google Cloud, as well as on-premises and other infrastructure. The tool can be used to create, update, and version infrastructure, making it easier to manage and track changes to resources over time.]


Terraform 
	- Terraform is an open source tool from HashiCorp company used to 
	  write Insfrastructure as A Code (IaC) to automate provisioning the public cloud insfrasructure.
  	
	- Terraform is written in Golang and available on GitHub 
	
	- Terraform code/configuration is written in HCL (HashiCorp Language) of type .tf
	
	- Terraform helps in prediction of changes through plans (terraform plan) and we can know what changes 
	  will be applied before applying it.

	- Terraform always keeps the infrastructure state saved and terraform at every apply 
	  of the changes to insfrasructure, is always matched with the state file (terraform.tfstate) and it predicts the changes 
	  need to be applied.
	  
	- Safe to run terraform code many times because terraform only apply the changes for 
	  the first if the chagnes are not there in state.

	- Terraform scripts can be version contralled means we keep the terraform 
	  scripts in github.
	  
Terraform Provider 
	- providers are the API to interact with public clouds, in our company aws is the 
	  provider that we are using.
	- In a complete terraform project we can have only one provider block.
		
			provider "aws" {
				access_key = ""
				secret_key = ""
				region = ""
			}
		
	- If we wont configure access_key and secret_key key in provider block then terraform 
	  will try to fetch the keys from aws_cli (taht is when we run aws -configure) that is using the below environment variables.
			1. AWS_ACCESS_KEY_ID =
			2. AWS_SECRET_ACCESS_KEY = 
			
	How to use multiple configurations with same provider block ?
	How to use a provider with multiple region ? 
		
	provider aliases
		- we can define multiple configuration for the same provider and select which provider
		  to use in resource and other blocks.
		
		provider "aws" {
			region = "ap-south-1"
		}
		
		provider "aws" {
			alias = "region1"
			region = "us-east-1"
		}

		provider "aws" {
			alias = "region2"
			region = "us-east-2"
		}		
		
		How to use provider alias ?
			resource "<resource_type>" "<resource_name>" {
				provider = aws.region1
			}
			
			resource "aws_instance" "my_instance" {
				provider = aws.region2
				ami = ""
				instance_type = "t2.micro"
			}
			
Terraform Registry 
	- Terraform registry is a repository of modules and resources which are written by trerraform 
	  w.r.t public clouds and we use this resources for provisioning the public clouds.
	- Terraform regisrty is also a community based registry which are maintained by providers only.
	
		URL: regisrty.terraform.io
		
	Example: aws - ec2 instance -> aws_instance
				 - s3 bucket    -> aws_s3_bucket 
				 - vpc 		    -> aws_vpc	
				 
Assignment: List all the resource types of aws provider which you know in aws ? 

Terraform init 
	- This command we use to initialize a working directory containing terraform script as 
	  terraform project. 
	- This in the first command that should be executed after writing a new terraform script.
        - It is safe to run this command multiple times. 
			
		The initializations steps are: 
			1. Initializing the Backend ....
			2. Initializing provider plugins ...
			3. Initializing the child modules ...
[The terraform init command is used to initialize a Terraform working directory. This command is typically run after writing or cloning Terraform configurations in order to download and install any required provider plugins, and to set up the backend for storing state. It is the first command that should be run when starting to use Terraform in a new working directory.
]
			
terraform plan 
	- the terraform plan is used to create a execution plan which internally performs a refresh 
	  unless user has disabled this refresh.
	- Plan will scan all the .tf scripts in the project directory and it manages to determine 
	  the changes to be applied to cloud (means it determines the desired state).
	- Plan will never do any changes to the actual insfrasructure it just gives what desired 
	  state will be applied.
	
	To save a plan in a file 
		terraform plan -out <file_name>	
		terraform plan -out plan.out	

terraform apply 
	- The terraform apply command is used to apply the planned changes to the actual cloud 
	  insfrasructure. 
	- This will internally run the plan again then with the user confirmation the predicted 
	  changes will be applied.
		
	To apply changes with out user confirmation 
		terraform apply --auto-approve
	
	To run a particular terraform script
		terraform apply <script>.tf <script1>.tf
		
	To apply a plan file                                                                [The plan file created using terraform plan -out plan.out]
		terraform apply <plan_file>
			
tarraform destroy 
	- This is used to destroy the resources which are manged by this current terraform 
	  scripts.
        - Crate a destroy plan

		To destroy one resource
			terraform destroy -target <resource_type>.<resource_name>
			example: terraform destroy -target aws_instance.ec2_ubuntu

terraform validate 
	- This is to validate the syntax of the terraform scripts in a project.
	
terraform show : This command inspects the file and displays resource details.

https://registry.terraform.io/providers/hashicorp/aws/latest/docs  :use documentation
------------------------------------------------------------------------------------------------------------------------------------------------------
Terraform State 
	Local - we can keep the state file locally in the same machine where we 
		execute the terraform scripts (It will be auto generated by terraform)
	
	Remote state (Backend) 
		Instead of keeping state file in local we can keep it in a remote location 
		such as s3, etcd. 
		
[ etcd is a distributed key-value store, which can be used as a database in certain use cases. It is designed for storing configuration data for distributed systems, and it can be used as a centralized store for storing data that needs to be shared across multiple systems or applications. ]

Terraform variable and outputs 
	Local variable 
		- A list of local variable can be declared in a terraform script and it scope 
		  is always within that script.
		- Instead of having same value defined multiple times in same script we can 
		  use local variable so that changing of value will be easy.
			locals {
				var1 = value1
				var2 = value2
				.
				.
				varn = valuen	
			}

			To access local variable we use local.<variable> keyword

	Input variables 
		This is a variable which we can use to pass parameters or user inputs 
		to terraform scripts.
		If we don't define default value then at the time of plan / apply terraform 
		will ask for value as user input in console.
	
			variable "<name>" {
				default = <default_value>
			}

	Output Variables 
	     - Output variables are like return values of terraform scripts.
	     - Output of a resource / module can be used as the input to another.
	     - To suppress the values from the console use sensitive = true 		
			output "<name>" {
			     value = aws_instance.instance1.public_ip	 	
			}

				or
			output "<name>" {
			     value = aws_instance.instance1.public_ip
			     sensitive = true
			}
		
Terraform module 
	All the terraform scripts in a folder is called a module 

	Root module 
	- The .tf files in the main working directory is root module where we have .terrform folder 	
	  or .tfstate 
	- Where we run terraform init 
	- Every terraform script should be in a module.

	Child module 
	- Terraform scripts defined in a sub-folder inside root module
	- We can segregate the scripts instead of writing everything in the same script.
	- We can reuse the child modules in other projects also.
	- These are always user defined 
	
----------------------------------------------------------------------------------------------------------------------------------------------------------------

Taint 
	- The terraform taint command is used to mark a terraform managed resource as tainted, which means 
	  the resource to be destroyed and recreated in the next apply.
	- Taint will not mark the actual resource in the cloud but it marks in the state file.
	
	To list resource in state 
		terraform state list
	To taint a resource 
		terraform taint <terra_arn>
		terraform taint aws_instance.example
        To remove the taint
		terraform untaint <terra_arn>
	The command to list tainted resources in Terraform is 
		terraform state list -module=module_name | grep "tainted".

	This command will list all the resources that are currently in a "tainted" state for a specific module. 
	Replace "module_name" with the name of the module you want to check.
	
  [	In this example, the "aws_instance.example" resource is being marked for recreation. When you run "terraform apply" next time, Terraform will destroy and   	    recreate this instance, while leaving other resources in your infrastructure untouched.
	
	You can also use terraform taint -module=MODULE_NAME RESOURCE_ADDRESS to taint a resource that is in a module.
        It's important to note that tainting a resource will not destroy the resource immediately, it will happen only when you run terraform apply next time.  ]



Provisioners 
	- Provisioners are used to copy files and directories, run command from terraform source machine to cloud 
	  resources as a part of resource creation.
	- They are similar to user-data scripts.
	- Most Provisioners requires authentication to run command via ssh or WinRM and we need to
	  define the connection.
	  
	 [ In Terraform, a "connection" block is used to configure the connection settings for a specific provider. 
	 It is typically used within a resource or data block and allows you to specify the connection details for the resource or data source you are creating.
	 
	 The "type" field specifies the type of connection, such as "ssh" or "api". 
	 The "host" field specifies the hostname or IP address of the resource or data source. 
	 The "user" and "password" fields are used to specify the username and password for the connection. 
	 The "private_key" field is used to specify the path to a private key file when using an SSH connection.
	 The syntax for a connection block is as follows:
		(connection is a sub-block)
		connection {
		     type = "ssh"
		     user = "root"
		     password = ""
		     host = "<ip>"	
		     private_key = "PRIVATE_KEY"
		}	
	

	file 
	    - The file Provisioner is used to copy file or directories from terraform project to cloud 
	      compute resource.
	    - This provisioner needs a connection to cloud resource.
	    	 	
	remote-exec 
	    - The remote-exec Provisioner is used to run command or script from terraform project to cloud 
	      compute resource.
	    - This provisioner needs a connection to cloud resource.


	1. ec2 instance with sg 	
	2. file provisioner - should find a file /home/ubuntu/text.txt
	3. remote-exec - Docker installed and file /home/ubuntu/remote-exec.txt
	4. local-exec - creates file Conf/ec2_ips.txt (with ec2 ip address)

[ Provisioners in Terraform are used to configure and provision resources after they have been created. 
They can be used to run scripts, copy files, or execute other actions on the created resources.

There are two types of provisioners in Terraform:

Local provisioners: These provisioners run on the machine where Terraform is executed, and are used to configure the resource after it has been created.
Remote provisioners: These provisioners run on the created resource, and are used to configure the resource after it has been created.
Provisioners are defined within a resource block and can be used to perform tasks like installing software, configuring settings, 
and running scripts on the created resource.  
Here's an example of how to use a local provisioner in Terraform:

resource "aws_instance" "example" {
  ami           = "ami-0c94855ba95c71c99"
  instance_type = "t2.micro"

  provisioner "local-exec" {
    command = "echo 'Hello, World!' > index.html"
  }
}

In this example, the local-exec provisioner is used to run a command on the local machine after the aws_instance resource has been created.
Here is an example of how to use a remote provisioner in Terraform:

resource "aws_instance" "example" {
  ami           = "ami-0c94855ba95c71c99"
  instance_type = "t2.micro"

  provisioner "remote-exec" {
    inline = [
      "sudo apt-get update",
      "sudo apt-get install -y apache2"
    ]
  }
}

In this example, the remote-exec provisioner is used to run commands on the created instance after it has been created.

A "file" provisioner in Terraform allows you to upload a local file to a remote machine created by Terraform as part of a provisioning process. 
This can be useful for uploading configuration files, scripts, or other files that are needed for the operation of the machine.

The syntax for a file provisioner block is as follows:

resource "resource_type" "resource_name" {
  ...
  provisioner "file" {
    source      = "path/to/local/file"
    destination = "path/to/remote/file"
    connection {
      type        = "ssh"
      host        = "hostname"
      user        = "username"
      password    = "password"
      private_key = "path/to/private/key"
    }
  }
}

The source field is the path to the local file that you want to upload, and the destination field is the path on the remote machine where the file should be uploaded.

You can also use the connection block to specify the connection details for the file provisioner, like so:
This is useful when the provisioner needs to connect to the remote machine over SSH.
Keep in mind that the file provisioner is executed after the Terraform resource is created, so the machine must be up and running, and the connection details must be valid for the file provisioner to work.

It's important to note that provisioners are executed only once, when the resource is created. 
You should use data resources and external data sources to perform actions on an already created resource.

null resource: 

In Terraform, a "null resource" is a special type of resource that allows you to perform arbitrary actions, such as running scripts or making API calls, without actually creating or managing any infrastructure. This can be useful when you need to perform actions that are not directly related to provisioning or managing infrastructure, such as sending notifications, triggering webhooks, or creating backups.
A null resource is defined using the null_resource block in Terraform configuration. 
It can have provisioners defined, which will be executed when the resource is created, updated or deleted.

resource "null_resource" "example" {
  provisioner "local-exec" {
    command = "echo 'Hello, World!' > index.html"
  }
}

It's important to note that null resources do not create or manage any infrastructure, they only run provisioners. 
Say for example: The requirement is to run provisioner block inside ec2-instance resource block, as we already have ec2-instance module defined I can use null_resource block, so that we can run provsioner as sub-block . null-resource means no resource is being created which means no change in the cloud configuration.

]


Terraform workspace 
	- workspace are a way of managing different set of statefiles which are independent to 
	  each other.
	- Every configuration should be in a workspace.
	- "default" is the default workspace in terraform 
	
	To list workspace
		terraform workspace list
	
	To create workspace 
		terraform workspace new <workspace_name>

	To switch between workspace
		terraform workspace select <workspace_name>
	
	To delete workspace 
		terraform workspace delete <workspace_name>

	Usage: 
	   -  We can deploy different version of terraform scripts with states associated with it.
	   -  We can maintain different set of infrastructure with same terraform repo with different 
	      state files.
	   -  We can deploy based on environment categorisation.

data source / data resource 
	- A data source is used to get the information dynamically from cloud.
	- This block is read-only and this will be always executed on every plan and apply.
	- we can query the required information and filter the field we want.

Null resource 		
	- The null resoruce will have the complete lifecycle support but there will be no change in the cloud configurations.
	- We can use triggers to collect the data of some resources.
	- We can use connections to connect ot resoruce using provisioners.
	- The triggers argument allows specifying an arbitory set of values only when changed, triiger will update the value.
	
		resoruce "null_resource" "<resource_name>" {
			
		}

null_data_resource 
	- This block we are not using in our company we use data block instead of this.
	
	data "null_data_source" "values" {
		  inputs = {
			all_server_ids = concat(aws_instance.green.*.id, aws_instance.blue.*.id)
			all_server_ips = concat(aws_instance.green.*.private_ip, aws_instance.blue.*.private_ip)
		  }
	}	


terraform settings 
	- used for specifying provider requirements. 
	- we can restrict the provider version.
	- we configure backend in this settings 
	- we use terraform block 
	- we can control the terraform version.
	- default file terraform.tf / versions.tf 	
		
		terraform {
			
		}

tf.vars vs variables.tf 
	variables.tf we can have variable block but variable blocks will not work in tf.vars
	tf.vars is to create variables whose scope will be at root module level.

	we use variables.tf to declare variables and we use tf.vars to asing value to variables.	
	
Terraform refresh 
	- Terraform refresh will find the updates done in the cloud resources by comparing the changes with state file.	
	- This won't modify your real remote objects, but it will modify the the Terraform state.


Terraform import  (Importing existing aws resource to terraform)
	- we are using terraforming tool in our company to import the existing configurations. 
	  Install terraforming in RHEL/Centos 
		(https://github.com/dtan4/terraforming)		
			sudo yum install rubygems
			sudo gem install terraforming

Terraform loops (count and for_each)	
	resource "null_resource" "loop_simple" {
		count = 2 
	}
	
	output "loop_out" {
		value = null_resource.loop_simple
	}
	
	
	List count example
	
	locals {
		names = ["bucket1","bucket2","bucket3","bucket4"]
	}
	
	resource "null_resource" "names" {
		count = length(local.names)
		triggers = {
			name = local.names[count.index]
		}
	}
	
	output "list_out" {
		value = null_resource.names
	}
	
	
	List for_each example
	
	locals {
		names = ["bucket1","bucket2","bucket3","bucket4"]
	}
	
	resource "null_resource" "names" {
		for_each = toset(local.names)
		triggers = {
			name = each.value
		}
	}
	
	output "list_out" {
		value = null_resource.names
	}
	
	Map for_each example
	
	locals {
		names = { 
			bucket1 = "region1"
			bucket2 = "region2"
			bucket3 = "region3"
			bucket4 = "region4"
		}
	}
	
	resoruce "null_resource" "names" {
		for_each = local.names
		triggers = {
			bucket = each.key
			region = each.value
		}
	}
	
	output "list_out" {
		value = null_resource.names
	}


Study Terragrunt by Gruntwork.io
